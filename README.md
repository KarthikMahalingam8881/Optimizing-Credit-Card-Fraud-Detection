# ğŸ’³ Cloud-Based Credit Card Fraud Detection Pipeline

This project converts a Jupyter-based machine learning model into a **modular, cloud-ready data engineering pipeline** using Python, AWS S3, and PostgreSQL. It enables automated data ingestion, transformation, and loading of credit card transaction data for downstream analysis or modeling.

---

## ğŸ§© Key Features

- âœ… Modular ETL structure (`extract.py`, `transform.py`, `load.py`, `main.py`)
- â˜ï¸ Cloud-native design with **S3 â†’ PostgreSQL** integration
- ğŸ§¼ Cleans and enriches raw data with fraud flags, Z-scores, and outlier markers
- ğŸ§ª Easily testable, scalable, and production-friendly
- ğŸ•¹ï¸ Fully runnable from the command line or scheduled via a workflow tool

---

## âš™ï¸ Tech Stack

- **AWS S3** â€“ Stores raw CSV files
- **boto3** â€“ Connects to S3 and fetches input data
- **Pandas** â€“ Used for cleaning, feature engineering
- **SQLAlchemy** â€“ Loads data into PostgreSQL
- **PostgreSQL** â€“ Stores queryable results
- **Python** â€“ Modularized ETL scripts
- *(Optional)* Airflow-compatible structure

---

## ğŸ” Pipeline Flow

S3 Bucket (raw/creditcard.csv)]

â†“

[extract.py] â€” Downloads raw data from S3

â†“

[transform.py] â€” Cleans, deduplicates, creates fraud flags

â†“

[load.py] â€” Loads enriched data into PostgreSQL


---

## ğŸ“ Project Structure


â”œâ”€â”€ extract.py # Downloads data from S3

â”œâ”€â”€ transform.py # Data cleaning + feature engineering

â”œâ”€â”€ load.py # Loads to PostgreSQL

â”œâ”€â”€ main.py # Orchestrates ETL

â”œâ”€â”€ config.py # Configs (DB URI, bucket name)

â”œâ”€â”€ requirements.txt

â”œâ”€â”€ .gitignore

â”œâ”€â”€ legacy_notebooks/ # Archived notebooks and PDFs

â””â”€â”€ README.md

---

## ğŸš€ How to Run the Pipeline

### 1. Install dependencies

pip install -r requirements.txt

### 2. Set your environment (recommended: .env file or export manually)
export AWS_ACCESS_KEY_ID=your_key

export AWS_SECRET_ACCESS_KEY=your_secret

export AWS_REGION=us-west-2

export S3_BUCKET=your-bucket-name

export DATABASE_URI=postgresql://username:password@host:port/creditcard_db


### 3. Run the pipeline
python main.py

###  Whatâ€™s in the Database

After running the pipeline, your PostgreSQL DB will have:

* A table: creditcard_data

* Cleaned and labeled records

* Ready for BI tools or modeling

# ğŸ—‚ï¸ Legacy Version
The original data science notebooks and PDFs are archived in:

/legacy_notebooks/

These contain exploratory work and notebook-based models for reference.

# ğŸ“Œ Why It Matters (from a Data Engineering POV)
* ğŸ“¦ Modular pipeline â€” clean separation of extract, transform, and load logic

* â˜ï¸ Cloud-native â€” integrates with AWS and PostgreSQL, can plug into Airflow or Lambda

* ğŸ’¡ Extensible â€” future support for logging, CI/CD, or scheduling

* ğŸ“ˆ BI/Analytics-ready â€” outputs clean, SQL-friendly tables


